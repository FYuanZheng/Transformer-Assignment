class Config:
    # Model Architecture
    d_model = 512          # æ¨¡å‹ç»´åº¦
    n_heads = 4            # æ³¨æ„åŠ›å¤´æ•°
    n_layers = 8           # Transformerå±‚æ•°
    d_ff = 2048            # FFNä¸­é—´å±‚ç»´åº¦ (é€šå¸¸æ˜¯4å€d_model)
    dropout = 0.3          # Dropoutç‡
    max_seq_len = 256      # æœ€å¤§åºåˆ—é•¿åº¦
    
    # Training
    batch_size = 6         # å•æ­¥batch size (è¯æ±‡é‡æ›´å¤§,è°ƒå°ä¸€ç‚¹)
    gradient_accumulation_steps = 6  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    effective_batch_size = batch_size * gradient_accumulation_steps  # æœ‰æ•ˆbatch size = 36
    
    learning_rate = 1e-4   # å­¦ä¹ ç‡
    weight_decay = 0.1    # æƒé‡è¡°å‡
    max_epochs = 4        # è®­ç»ƒè½®æ•°
    warmup_steps = 500     # å­¦ä¹ ç‡warmupæ­¥æ•°


    # Early Stopping
    early_stopping = True  # å¯ç”¨æ—©åœ
    patience = 5           # éªŒè¯lossè¿ç»­5ä¸ªepochä¸ä¸‹é™åˆ™åœæ­¢
    
    # Learning Rate Scheduling
    use_scheduler = True   # ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_decay_factor = 0.5  # å­¦ä¹ ç‡è¡°å‡å› å­
    lr_patience = 3        # éªŒè¯lossè¿ç»­3ä¸ªepochä¸ä¸‹é™åˆ™é™ä½å­¦ä¹ ç‡
    
    # Gradient Clipping
    max_grad_norm = 0.5    # ğŸ”§ æ¢¯åº¦è£å‰ª (1.0â†’0.5, æ›´æ¿€è¿›)
    
    # Device
    device = 'cuda'        # ä½¿ç”¨GPU
    use_amp = True         # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    
    # Data
    data_dir = './wikitext-2'  # æ•°æ®é›†ç›®å½•
    vocab_size = None      # å°†åœ¨æ•°æ®åŠ è½½åè®¾ç½®
    
    # Logging
    log_interval = 50      # æ¯å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡
    eval_interval = 500    # æ¯å¤šå°‘æ­¥éªŒè¯ä¸€æ¬¡
    
    def __repr__(self):
        return f"""
Config (Regularized):
  Model: d_model={self.d_model}, n_heads={self.n_heads}, n_layers={self.n_layers}
  Regularization: dropout={self.dropout}, weight_decay={self.weight_decay}, grad_clip={self.max_grad_norm}
  Training: batch_size={self.batch_size}, grad_accum={self.gradient_accumulation_steps}, lr={self.learning_rate}
  Early Stopping: {self.early_stopping} (patience={self.patience})
  LR Scheduler: {self.use_scheduler} (factor={self.lr_decay_factor}, patience={self.lr_patience})
  Sequence Length: {self.max_seq_len}
  Epochs: {self.max_epochs}
  Device: {self.device}, AMP: {self.use_amp}
"""