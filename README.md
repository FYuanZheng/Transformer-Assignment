# Decoder-Only Transformer è¯­è¨€æ¨¡å‹

æ‰‹å†™å®ç°çš„ Decoder-only Transformerï¼Œä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç (RoPE)å’ŒGELUæ¿€æ´»å‡½æ•°ï¼Œåœ¨ WikiText-2 æ•°æ®é›†ä¸Šè®­ç»ƒã€‚

## ğŸŒŸ ç‰¹æ€§

- âœ… **æ‰‹å†™å®ç° Transformer**ï¼šä¸ä½¿ç”¨ `torch.nn.Transformer`
- âœ… **æ—‹è½¬ä½ç½®ç¼–ç (RoPE)**ï¼šæ›¿ä»£ä¼ ç»Ÿçš„ç»å¯¹ä½ç½®ç¼–ç 
- âœ… **GELU æ¿€æ´»å‡½æ•°**ï¼šç°ä»£ Transformer æ ‡é…
- âœ… **Qwen Tokenizer**ï¼šä½¿ç”¨å·¥ä¸šçº§åˆ†è¯å™¨
- âœ… **4GB æ˜¾å­˜ä¼˜åŒ–**ï¼šæ··åˆç²¾åº¦è®­ç»ƒ + æ¢¯åº¦ç´¯ç§¯
- âœ… **å®Œæ•´è®­ç»ƒæµç¨‹**ï¼šå¸¦è¿›åº¦æ¡å’Œå¯è§†åŒ–

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ config.py          # é…ç½®æ–‡ä»¶ï¼ˆè¶…å‚æ•°ï¼‰
â”œâ”€â”€ model.py           # Transformeræ¨¡å‹å®šä¹‰
â”œâ”€â”€ data.py            # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
â”œâ”€â”€ train.py           # è®­ç»ƒä¸»ç¨‹åº
â”œâ”€â”€ ablation.py        # æ¶ˆèå®éªŒ
â”œâ”€â”€ requirements.txt   # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ wikitext-2/        # æ•°æ®é›†ç›®å½•
    â”œâ”€â”€ wiki.train.tokens (æˆ– .txt)
    â”œâ”€â”€ wiki.valid.tokens (æˆ– .txt)
    â””â”€â”€ wiki.test.tokens  (æˆ– .txt)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰
conda create -n transformer python=3.10
conda activate transformer

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æ³¨æ„ï¼šPyTorch éœ€è¦æ ¹æ®ä½ çš„CUDAç‰ˆæœ¬å®‰è£…
# è®¿é—® https://pytorch.org/ è·å–æ­£ç¡®çš„å®‰è£…å‘½ä»¤
```

### 2. å‡†å¤‡æ•°æ®é›†

ä¸‹è½½ WikiText-2 æ•°æ®é›†å¹¶æ”¾ç½®åˆ° `./wikitext-2/` ç›®å½•ï¼š
- [WikiText-2 ä¸‹è½½é“¾æ¥](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip)

æ•°æ®é›†ä¹Ÿå·²åŒ…å«åœ¨é¡¹ç›®ä¸­ä¸Šä¼ ï¼Œä¸ç”¨å†æ¬¡ä¸‹è½½ã€‚

### 3. å¼€å§‹è®­ç»ƒ

```bash
python train.py
```

## âš™ï¸ é…ç½®è¯´æ˜

ä¸»è¦é…ç½®åœ¨ `config.py` ä¸­ï¼š

```python
# æ¨¡å‹æ¶æ„
d_model = 256          # æ¨¡å‹ç»´åº¦
n_heads = 4            # æ³¨æ„åŠ›å¤´æ•°
n_layers = 6           # Transformerå±‚æ•°
max_seq_len = 128      # åºåˆ—é•¿åº¦

# è®­ç»ƒå‚æ•°
batch_size = 2         # å•æ­¥batch
gradient_accumulation_steps = 16  # æ¢¯åº¦ç´¯ç§¯
max_epochs = 20        # è®­ç»ƒè½®æ•°
learning_rate = 3e-4   # å­¦ä¹ ç‡
```

### ğŸ’¾ æ˜¾å­˜ä¼˜åŒ–é…ç½®

| é…ç½® | 4GBæ˜¾å­˜ | 8GBæ˜¾å­˜ | 16GBæ˜¾å­˜ |
|------|---------|---------|----------|
| d_model | 256 | 512 | 768 |
| n_layers | 6 | 8 | 12 |
| batch_size | 2 | 8 | 16 |
| max_seq_len | 128 | 256 | 512 |

## ğŸ“Š è®­ç»ƒè¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- å®æ—¶è¿›åº¦æ¡ï¼ˆæ¯ä¸ªbatchï¼‰
- æ¯ä¸ªepochçš„ Loss å’Œ Perplexity
- è®­ç»ƒæ›²çº¿å›¾ `training_curves.png`

ç¤ºä¾‹è¾“å‡ºï¼š
```
Epoch 1/20 | Time: 120.5s
Train Loss: 5.2341 | Train PPL: 187.23
Valid Loss: 4.8912 | Valid PPL: 132.67
âœ“ New best validation loss!
```

## ğŸ¯ æ¨¡å‹æ¶æ„ç»†èŠ‚

### RoPE (æ—‹è½¬ä½ç½®ç¼–ç )
```python
# ç›¸æ¯”ç»å¯¹ä½ç½®ç¼–ç çš„ä¼˜åŠ¿ï¼š
- æ›´å¥½çš„é•¿åº¦å¤–æ¨èƒ½åŠ›
- ç›¸å¯¹ä½ç½®ä¿¡æ¯ç¼–ç 
- ç°ä»£LLMæ ‡é…ï¼ˆGPT-NeoX, LLaMAç­‰ï¼‰
```

### Pre-LN Transformer
```python
# Layerç»“æ„:
x = x + Attention(LayerNorm(x))
x = x + FFN(LayerNorm(x))
```

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

WikiText-2 æ•°æ®é›†ä¸Šçš„å‚è€ƒæŒ‡æ ‡ï¼š

| Epochs | Valid PPL | è¯´æ˜ |
|--------|-----------|------|
| 5 | 150-200 | åˆæ­¥æ”¶æ•› |
| 10 | 80-120 | è‰¯å¥½æ•ˆæœ |
| 20 | 50-80 | è¾ƒä¼˜æ•ˆæœ |

**æ³¨**: å°æ¨¡å‹é…ç½®(d_model=256)çš„PPLä¼šæ¯”å¤§æ¨¡å‹é«˜ï¼Œè¿™æ˜¯æ­£å¸¸çš„ã€‚

## ğŸ”§ æ•…éšœæ’æŸ¥

### OOM (æ˜¾å­˜ä¸è¶³)
```bash
# æ–¹æ³•1: å‡å°batch size
batch_size = 1

# æ–¹æ³•2: å‡å°åºåˆ—é•¿åº¦
max_seq_len = 64

# æ–¹æ³•3: å‡å°æ¨¡å‹
d_model = 128
n_layers = 4
```

### Tokenizerä¸‹è½½å¤±è´¥
```python
# åœ¨ data.py ä¸­åˆ‡æ¢åˆ° GPT-2 tokenizer:
tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

### CUDAä¸å¯ç”¨
```python
# åœ¨ config.py ä¸­æ”¹ä¸ºCPUè®­ç»ƒ:
device = 'cpu'
use_amp = False  # CPUä¸æ”¯æŒæ··åˆç²¾åº¦
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - åŸå§‹Transformerè®ºæ–‡
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) - RoPEè®ºæ–‡
- [WikiText-2 Dataset](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/)

## ğŸ“„ License

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼
